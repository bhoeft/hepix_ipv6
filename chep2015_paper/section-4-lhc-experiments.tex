

The shortage of available IPv4 addresses implies that there is a significant possibility that new large
computing facilities will not be able to give IPv4 addresses to all of the machines in their network. The
most likely consequence is that for these sites, worker nodes -- which constitute the largest fraction of
independent computing nodes will have purely IPv6 network addresses. Hence, the main use case for the LHC
experiments is to enable jobs to run on these machines, access their software areas and input data and
upload their outputs to various grid storages or services as needed.

The LHC experiments generally assume \cite{LHCassumption} that the storage on different sites and
supporting middleware \cite{middleware} like the LFC will either be directly dual-stack, or support
dual stack operation in some way, enabling seamless access to the storage as needed for either downloading
or saving. For example, it is expected that dual-stack squid proxies will be needed for CVMFS and xrootd
will soon be dual-stack, to handle storage technologies like Castor which will be IPv4 only. The
servers that the LHC experiments use to handle the grid infrastructure are / will also be dual-stack.

As an example of the above, we look at LHCb \cite{LHCb} which is the experiment on the LHC, optimised
for studying beauty and charm physics. LHCb uses the DIRAC \cite{DIRAC} interware to manage its grid
operations. The DIRAC software was coded to be able to handle both IPv4 and IPv6 addresses in late 2014,
with the modifications being easy enough to make by non-expert programmers. While testing the processes
on a dual-stack machine, it was found that there was a significant number of connections which were not
going through to the servers. This was finally traced back to a missing python compiler "enable\_ipv6" option in an external library thereby causing errors in identifying IPv6 addresses. Using a new version of the library, the
problem with dropped connections went away and testing DIRAC will restart soon.

In general, testing of the grid middleware by the different experiments is going ahead as fast as possible
given the manpower and time constraints of the LHC startup and the immediate issues with handling purely
IPv6 worker nodes are expected to be sorted by sometime in 2016.


